\section*{Abstract}
\label{sec:abstract}
We examine whether alignment effects decay across multi-turn conversations, causing large language models to regress toward a base prior that is more willing to comply with harmful requests. This question matters because most real interactions are multi-turn, yet turn-indexed safety evidence remains limited. We evaluate \gptfourone on controlled turn-count variants of two safety datasets (\jbb and \wildjb) and a benign multi-turn benchmark (\mtbench). For safety prompts, we compare a single-turn baseline (turn count 1) to a three-turn sequence with benign lead-ins and the same final harmful request. For benign tasks, we compare turn-1 and turn-2 success on \mtbench. We find no increase in harmful compliance on \jbb (0.05 at both turn counts), and a substantial \textit{decrease} in compliance on \wildjb (0.75 to 0.35; Wilcoxon $p=0.0047$, mean difference $-0.40$). Benign task success drops slightly on \mtbench (0.80 to 0.75) without significance ($p=0.655$). These results do not support prior-regression in this small sample and instead suggest that multi-turn context can strengthen refusal behavior. Our study provides a simple, reproducible protocol for turn-indexed evaluation and highlights the need for larger-scale and attack-specific ablations.
